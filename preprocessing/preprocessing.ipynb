{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe6294e",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Machine Learning\n",
    "\n",
    "##### by MÃ¡rk Kereszty\n",
    "*Note: This Jupyter Notebook is optimised for the **Design and development of an IoT-based data collection/data analysis system** thesis project!*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffa470",
   "metadata": {},
   "source": [
    "To use the collected data for model training in the field of Machine Learning, we need to make some preparations first. These are so-called preprocessing steps, that can vary based on the input and the desired output. This notebook contains the following steps:\n",
    "- converting .csv dataset to Pandas DataFrame\n",
    "- dropping irrelevant columns\n",
    "- grouping measurements based on the timestamps\n",
    "- converting strings to floats and extracting necessary information\n",
    "- filling in the missing values\n",
    "- scaling the dataset for optimal value range\n",
    "\n",
    "These steps can be built into a preprocessing pipeline in scikit-learn, but for now let's go over each one separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e54dd",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "First lets select the data you want to process. This should be a file with *.csv* extension, downloaded from the database via the [OpenDAQ dashboard]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224ecaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "file_path = filedialog.askopenfilename()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4d102",
   "metadata": {},
   "source": [
    "Now that you have selected the source, the next step is to load it into a [Pandas](https://pandas.pydata.org/) DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2100973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 228 records across 12 variables.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Loaded {len(df.index)} records across {len(df['variable'].unique())} variables.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607eef9",
   "metadata": {},
   "source": [
    "### Restructuring the Data\n",
    "\n",
    "Since *_result, table, _measurement, aspect, org, sensor, student and topic* columns don't contain relevant information in the scope of ML, we can go ahead and drop them.\n",
    "The next step is to organize the data. With the help of the timestamps we can determine which records are correlated. After grouping them by timestamp we can combine each type of data into a single row. This way the measurements ar logically organised and more manageable. Finally we can also drop the columns ending in *_fft*, the PSD columns hold more value in regards of vibration analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9596ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['result', 'table', '_measurement', 'aspect', 'org', 'sensor', 'student', 'topic']\n",
    "df.drop(labels=labels, axis=1, inplace=True)\n",
    "\n",
    "# grouping by timestamp\n",
    "df['_time'] = pd.to_datetime(df['_time'])\n",
    "df = df.sort_values('_time')\n",
    "\n",
    "grouped_measurements = []\n",
    "current_group = []\n",
    "current_time = None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if current_time is None:\n",
    "        current_time = row['_time']\n",
    "        current_group.append(row)\n",
    "    elif (row['_time'] - current_time) <= timedelta(seconds=1):\n",
    "        current_group.append(row)\n",
    "    else:\n",
    "        grouped_measurements.append(current_group)\n",
    "        current_group = [row]\n",
    "        current_time = row['_time']\n",
    "\n",
    "# appending the last group\n",
    "grouped_measurements.append(current_group)\n",
    "\n",
    "# new dataframe from the grouped values\n",
    "result_data = {'_time': [group[0]['_time'] for group in grouped_measurements]}\n",
    "for variable in df['variable'].unique():\n",
    "    result_data[variable] = [next((row['_value'] for row in group if row['variable'] == variable), None) for group in grouped_measurements]\n",
    "\n",
    "df = pd.DataFrame(result_data)\n",
    "\n",
    "# droping fft columns\n",
    "for column in df.columns:\n",
    "    if 'fft' in column:\n",
    "        df.drop(labels=column, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ccc857",
   "metadata": {},
   "source": [
    "### Strings to Numbers\n",
    "\n",
    "Many values are stored as strings inside InfluxDB, since they are lists. First we need to convert them back to lists of floats using the *ast* library. For the phase currents we can calculate the sum and store them instead of separate current values. The RMS values are also strings because they are part of the *lathe_analytics* measurement group and the other two variables (fft and psd) are strings. These only need to be converted back to float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbade3",
   "metadata": {},
   "source": [
    "Lastly we have the PSD data. The method I choose here is to only store the *n* highest power values with their corresponding frequencies, for each axis. This way we can recognise patterns while keeping the dataset manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e0d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the cur column to sum of currents\n",
    "df['cur'] = df['cur'].apply(lambda x: sum(ast.literal_eval(x)) if x is not None else None)\n",
    "\n",
    "# converting rms column from str to float\n",
    "for column in df.columns:\n",
    "    if 'rms' in column:\n",
    "        df[column] = df[column].apply(lambda x: float(x) if x is not None else None)\n",
    "\n",
    "\n",
    "\n",
    "def process_psd(original_df, column, count=10):\n",
    "    column_names = [f'{column[:4]}_pow_{i+1}' for i in range(count)]\n",
    "    column_names += [f'{column[:4]}_freq_{i+1}' for i in range(count)]\n",
    "    df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    for index, row in original_df.iterrows():\n",
    "        try:\n",
    "            ls = ast.literal_eval(row[column])\n",
    "        \n",
    "            sorted_data = sorted(zip(ls[1], ls[0]), key=lambda x: x[0], reverse=True)[:count]\n",
    "            data = [i for i, j in sorted_data] + [j for i, j in sorted_data]\n",
    "\n",
    "            top_df = pd.DataFrame(np.array(data).reshape(-1,len(data)), columns=column_names)\n",
    "            #print(top_10_df)\n",
    "            df = pd.concat([df, top_df], axis=0, ignore_index=True)\n",
    "        except:\n",
    "            top_df = pd.DataFrame(np.array([None for _ in range(count*2)]).reshape(-1,len(data)), columns=column_names)\n",
    "            df = pd.concat([df, top_df], axis=0, ignore_index=True)\n",
    "    return df\n",
    "        \n",
    "\n",
    "\n",
    "# get the top frequencies and the corresponding power value in separate cells\n",
    "psd_df = pd.DataFrame()\n",
    "for column in df.columns:\n",
    "    if 'psd' in column:\n",
    "        psd_df = pd.concat([psd_df, process_psd(df, column, 10)], axis=1)\n",
    "\n",
    "df = pd.concat([df, psd_df], axis=1)\n",
    "\n",
    "df.drop(['_time'], axis=1, inplace=True)\n",
    "for column in df.columns:\n",
    "    if 'psd' in column:\n",
    "        df.drop(labels=column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9786e0",
   "metadata": {},
   "source": [
    "### Dealing With Missing Data\n",
    "\n",
    "Sometimes there can be missing values. These rows can be either deleted, although that can lead to inconsitency, or they can be estimated using different methods. In this case I used an Imputer from [scikit-learn](https://scikit-learn.org/stable/) to calculate the mean of the DataFrame column and fill in the missing values with the help of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d64b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "for column in df.columns:\n",
    "    if df[column].isna().any():\n",
    "        df[column] = imputer.fit_transform(df[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7838d8",
   "metadata": {},
   "source": [
    "If you want to inspect the transformed dataset, you can export it to *.csv* and view it in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57d81e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "#df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb1442",
   "metadata": {},
   "source": [
    "### Scaling/Standardization\n",
    "\n",
    "Standardization is an important step in preprocessing: many ML models need datasets that have features following a near Gaussian distribution with zero mean and unit variance. For this reason we can use the *StandardScaler* from scikit-learn to fit a scaler to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9388609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.34395419e-16  1.05179023e-16  1.57768535e-16 -3.64620614e-15\n",
      "  1.09853647e-14 -8.22295448e-15  1.75298372e-16 -7.94503352e-16\n",
      " -1.25849623e-15  1.15404762e-16  6.23770041e-16 -8.79413501e-16\n",
      "  3.12615431e-16  5.12747739e-16 -1.25630500e-16  6.83663652e-16\n",
      "  5.84327908e-17 -9.34924652e-17 -4.67462326e-17  2.16201326e-16\n",
      "  7.88842675e-17  0.00000000e+00 -5.25895117e-17  5.84327908e-17\n",
      "  5.84327908e-17 -1.16865582e-16  0.00000000e+00  5.25895117e-17\n",
      " -6.77820373e-16  1.24754008e-15  5.87249547e-16  9.86053344e-16\n",
      " -7.24566606e-16  4.35324291e-16  8.76491862e-17  1.58571986e-15\n",
      "  5.27721142e-17  3.50596745e-17  5.84327908e-18  1.66533454e-16\n",
      " -4.67462326e-17 -5.84327908e-17  6.42760698e-17  6.42760698e-17\n",
      " -1.75298372e-17  0.00000000e+00  2.86320675e-16 -2.72881133e-15\n",
      "  6.77820373e-16 -7.12880047e-16  2.07436407e-16 -1.17376868e-15\n",
      "  8.47275466e-16  1.45935895e-15 -2.15032670e-15 -1.51925256e-16\n",
      "  7.01193489e-17 -6.86585292e-17  5.84327908e-17 -5.11286919e-17\n",
      "  8.18059071e-17 -1.22708861e-16 -1.05179023e-16  3.06772152e-17\n",
      "  1.63611814e-16  4.67462326e-17]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(df)\n",
    "df_scaled = scaler.transform(df)\n",
    "\n",
    "print(df_scaled.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dc6a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(df_scaled.std(axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
